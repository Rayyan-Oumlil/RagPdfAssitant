# ğŸ§  RAG PDF Assistant

A local AI assistant that lets you upload **PDF files** and ask **questions** about them using **RAG (Retrieval-Augmented Generation)**. It runs **fully offline** using a local LLM like Mistral via [Ollama](https://ollama.com), and uses `FAISS` + `sentence-transformers` for semantic search.

> Built with FastAPI, PyMuPDF, FAISS, and Mistral LLM.

---

## ğŸ“¦ Features

- ğŸ” Extracts text from PDF files
- ğŸ§  Splits and embeds document chunks
- ğŸ” Retrieves relevant content using vector search (FAISS)
- ğŸ¤– Answers questions using a local LLM (Mistral)
- ğŸ’¾ Works fully offline (no OpenAI API needed)
- ğŸ”§ Easy to extend with Streamlit, React, or Docker

---

## ğŸš€ Tech Stack

| Category   | Tools Used                           |
|------------|---------------------------------------|
| Backend    | FastAPI, Python                      |
| AI / NLP   | Mistral (via Ollama), SentenceTransformers |
| Embeddings | all-MiniLM-L6-v2                     |
| Vector DB  | FAISS                                |
| File I/O   | PyMuPDF                              |
| Optional   | Streamlit or React (for UI)          |

---

## ğŸ“ Project Structure

rag-assistant/
â”œâ”€â”€ backend/ # FastAPI backend
â”‚ â”œâ”€â”€ main.py # API endpoints
â”‚ â”œâ”€â”€ rag.py # RAG logic (embedding + retrieval)
â”‚ â””â”€â”€ utils.py # PDF parsing + chunking
â”œâ”€â”€ data/ # Uploaded files & FAISS index
â”œâ”€â”€ models/ # Optional LLM/config files
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ .gitignore # Ignore cache, env, FAISS index
â””â”€â”€ README.md # You're here!

yaml
Copier
Modifier

---

## âš™ï¸ Requirements

- Python 3.10+
- [Ollama](https://ollama.com) (for local LLMs)
- Git, pip

---

## ğŸ§ª Setup Instructions

### 1. Clone the Repo

```bash
git clone https://github.com/Rayyan-Oumlil/RagPdfAssitant.git
cd RagPdfAssitant
2. Create a Virtual Environment (optional)
bash
Copier
Modifier
python -m venv venv
venv\Scripts\activate  # Windows
3. Install Dependencies
bash
Copier
Modifier
pip install -r requirements.txt
4. Start Ollama + Download the Mistral Model
bash
Copier
Modifier
ollama run mistral
The first time, it will download ~4.1GB. Keep this terminal open.

5. Start the FastAPI Server
Open another terminal and run:

bash
Copier
Modifier
uvicorn backend.main:app --reload
Visit the docs at:
ğŸ“ http://127.0.0.1:8000/docs

ğŸ“¡ API Endpoints
â• POST /upload
Upload and index a PDF file

Form field: file: UploadFile

â“ POST /ask
Ask a question about the uploaded content

Form field: question: str

ğŸ§  How It Works (RAG Flow)
Upload a PDF

Text is extracted and split into small chunks

Chunks are embedded with sentence-transformers

Embeddings stored in FAISS

When a question is asked:

It's embedded

Top chunks are retrieved from FAISS

Chunks + question are passed to the LLM (Mistral via Ollama)

A natural language answer is generated

ğŸ§± Python Dependencies
nginx
Copier
Modifier
fastapi
uvicorn
pymupdf
sentence-transformers
faiss-cpu
python-multipart
ğŸ›¡ï¸ Limitations
Not production-ready (no auth, rate-limiting, or sanitization)

Currently only supports .pdf files

Mistral output quality may vary; no fine-tuning applied

âœ… TODO / Improvements
 Add Streamlit or React UI

 Add citations and source highlighting

 Support multiple documents / users

 Dockerize the project

 Add .txt file support

ğŸ‘¨â€ğŸ’» Author
Built by Rayyan Oumlil

ğŸ“ License
MIT License. Free to use, modify, and share.

yaml
Copier
Modifier

---

Once you add this file to your repo and commit:

```bash
git add README.md
git commit -m "Add full README"
git push origin main
